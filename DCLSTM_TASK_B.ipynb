{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCLSTM_TASK_B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basakstuff/DeepConvLSTM/blob/main/DCLSTM_TASK_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P__1a6Ny-b60"
      },
      "source": [
        "# **Task B2: Multimodal activity recognition: Gestures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwFQc7vlDHa8"
      },
      "source": [
        "Including Relevent Library files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiMgX0XtkESl"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import time\n",
        "import pickle as cp\n",
        "from sliding_window import sliding_window"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8isyaxJCDkY"
      },
      "source": [
        "Loading Opportunity Dataset and including accessory files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA3pSs2bjrmb"
      },
      "source": [
        "#Loading Opportunity Dataset.\n",
        "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip --no-check-certificate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPqyTT-ejvhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9e276a-f525-400c-cd6f-608707fada60"
      },
      "source": [
        "#Including preprocessing file.\n",
        "!python preprocess_data.py -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: preprocess_data.py [-h] -i INPUT -o OUTPUT [-t {gestures,locomotion}]\n",
            "\n",
            "Preprocess OPPORTUNITY dataset\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -i INPUT, --input INPUT\n",
            "                        OPPORTUNITY zip file\n",
            "  -o OUTPUT, --output OUTPUT\n",
            "                        Processed data file\n",
            "  -t {gestures,locomotion}, --task {gestures,locomotion}\n",
            "                        Type of activities to be recognized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuxgJZMy9q-1"
      },
      "source": [
        "## **TASK B :- Applying Model on Gestures (with NULL Class)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X8w_bePdVKl"
      },
      "source": [
        "Preprocessing the data from data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Plu9za-rbAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d454e8-0e69-4053-f333-07f9f8566f9d"
      },
      "source": [
        "!python preprocess_data.py -i data/OpportunityUCIDataset.zip -o oppChallenge_gestures.data -t gestures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking dataset data/OpportunityUCIDataset.zip\n",
            "Processing dataset files ...\n",
            "... file OpportunityUCIDataset/dataset/S1-Drill.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL1.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL2.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL3.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL4.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL5.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-Drill.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL1.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL2.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL3.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-Drill.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL1.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL2.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL3.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL4.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL5.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL4.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL5.dat\n",
            "Final datasets with size: | train (557963, 113) | test (118750, 113) | \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAPgfCuzry_E"
      },
      "source": [
        "# Hardcoded number of sensor channels employed in the OPPORTUNITY challenge\n",
        "NB_SENSOR_CHANNELS = 113\n",
        "\n",
        "# Hardcoded number of classes in the gesture recognition problem\n",
        "NUM_CLASSES = 18\n",
        "\n",
        "# Hardcoded length of the sliding window mechanism employed to segment the data\n",
        "SLIDING_WINDOW_LENGTH = 24\n",
        "\n",
        "\n",
        "# Hardcoded step of the sliding window mechanism employed to segment the data\n",
        "SLIDING_WINDOW_STEP = 12\n",
        "\n",
        "# Batch Size\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# Number filters convolutional layers\n",
        "NUM_FILTERS = 64\n",
        "\n",
        "# Size filters convolutional layers\n",
        "FILTER_SIZE = 5\n",
        "\n",
        "# Number of unit in the long short-term recurrent layers\n",
        "NUM_UNITS_LSTM = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGkixGUydVKm"
      },
      "source": [
        "Loading the data and segmenting it according to length of sliding window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w5kuIMDr5Qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2847e1f-d2c1-4fe0-cc51-3ff791f58e68"
      },
      "source": [
        "def load_dataset(filename):\n",
        "\n",
        "    f = open(filename, 'rb')\n",
        "    data = cp.load(f)\n",
        "    f.close()\n",
        "\n",
        "    X_train, y_train = data[0]\n",
        "    X_test, y_test = data[1]\n",
        "\n",
        "    print(\" ..from file {}\".format(filename))\n",
        "    print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n",
        "\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "    # The targets are casted to int8 for GPU compatibility.\n",
        "    y_train = y_train.astype(np.uint8)\n",
        "    y_test = y_test.astype(np.uint8)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "print(\"Loading data...\")\n",
        "X_train, y_train, X_test, y_test = load_dataset('data/oppChallenge_gestures.data')\n",
        "\n",
        "assert NB_SENSOR_CHANNELS == X_train.shape[1]\n",
        "def opp_sliding_window(data_x, data_y, ws, ss):\n",
        "    data_x = sliding_window(data_x,(ws,data_x.shape[1]),(ss,1))\n",
        "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,ws,ss)])\n",
        "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n",
        "\n",
        "# Sensor data is segmented using a sliding window mechanism\n",
        "X_test, y_test = opp_sliding_window(X_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
        "print(\" ..after sliding window (testing): inputs {0}, targets {1}\".format(X_test.shape, y_test.shape))\n",
        "\n",
        "# Data is reshaped since the input of the network is a 4 dimension tensor\n",
        "X_test = X_test.reshape((-1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            " ..from file data/oppChallenge_gestures.data\n",
            " ..reading instances: train (557963, 113), test (118750, 113)\n",
            " ..after sliding window (testing): inputs (9894, 24, 113), targets (9894,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5_HUMPVr_aJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22945a06-5bd3-42ca-ec9c-b03abf4cc65e"
      },
      "source": [
        "X_train, y_train = opp_sliding_window(X_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
        "print(\" ..after sliding window (training): inputs {0}, targets {1}\".format(X_train.shape, y_train.shape))\n",
        "X_train = X_train.reshape((-1,SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS,1))\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ..after sliding window (training): inputs (46495, 24, 113), targets (46495,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46495, 24, 113, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA_Z98xG8Qdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1360019d-8691-479e-d2ef-36bdeaca8eac"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46495, 24, 113, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu94WSCAdVKm"
      },
      "source": [
        "Defining DeepConvLSTM Model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGj-7dtQsPLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28eae90-159b-4459-e0f4-242576c0a341"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS,1)))\n",
        "initializer = tf.keras.initializers.Orthogonal()\n",
        "model.add(layers.Conv2D(NUM_FILTERS, kernel_size=(FILTER_SIZE, 1), activation=\"relu\",kernel_initializer=initializer))\n",
        "model.add(layers.Conv2D(NUM_FILTERS, kernel_size=(FILTER_SIZE, 1), activation=\"relu\",kernel_initializer=initializer))\n",
        "model.add(layers.Conv2D(NUM_FILTERS, kernel_size=(FILTER_SIZE, 1), activation=\"relu\",kernel_initializer=initializer))\n",
        "model.add(layers.Conv2D(NUM_FILTERS, kernel_size=(FILTER_SIZE, 1), activation=\"relu\",kernel_initializer=initializer))\n",
        "model.add(layers.Permute((2,1,3)))\n",
        "model.add(layers.Reshape( (int(model.layers[4].output_shape[1]), int(model.layers[4].output_shape[2]) * int(model.layers[4].output_shape[3]))))\n",
        "model.add(layers.LSTM(NUM_UNITS_LSTM,return_sequences=True))\n",
        "model.add(layers.LSTM(NUM_UNITS_LSTM,return_sequences=True))\n",
        "#model.add(layers.Reshape( (-1, NUM_UNITS_LSTM)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "    \n",
        "#Printing Model Summary.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 20, 113, 64)       384       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 113, 64)       20544     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 113, 64)       20544     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 113, 64)        20544     \n",
            "_________________________________________________________________\n",
            "permute (Permute)            (None, 113, 8, 64)        0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 113, 512)          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 113, 128)          328192    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 113, 128)          131584    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 14464)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 18)                260370    \n",
            "=================================================================\n",
            "Total params: 782,162\n",
            "Trainable params: 782,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaNO5DoDt_MH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ae7110-12a3-4cb2-bdab-02867e19f76d"
      },
      "source": [
        "model.layers[4].output_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 113, 8, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0WHzEx5dVKn"
      },
      "source": [
        "Encoding the training and testing data to One-Hot Encoded form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp5zuMCyBJ7n"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "def prepare_targets(y_train, y_test):\n",
        "\tohe = OneHotEncoder()\n",
        "\tohe.fit(y_train)\n",
        "\ty_train_enc = ohe.transform(y_train)\n",
        "\ty_test_enc = ohe.transform(y_test)\n",
        "\treturn y_train_enc.A, y_test_enc.A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbmz5JJyBNAL"
      },
      "source": [
        "y_train=y_train.reshape(-1,1)\n",
        "y_test=y_test.reshape(-1,1)\n",
        "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybj5zXlidVKn"
      },
      "source": [
        "Compiling Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNAA6mqD12N7"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd6ubHccdVKn"
      },
      "source": [
        "Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0pRaMrtxOe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95feae1e-23e8-4478-d4f6-2decf64ff061"
      },
      "source": [
        "model.fit(X_train, y_train_enc, batch_size=BATCH_SIZE, epochs=50,validation_split=0.1)\n",
        "model.save(\"B_Null_Basak.h5\") #MODEL_NAME.h5\n",
        "\n",
        "#Model is already trained and loaded directly for testing data."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "419/419 [==============================] - 65s 141ms/step - loss: 1.5771 - categorical_accuracy: 0.6681 - val_loss: 0.8032 - val_categorical_accuracy: 0.8144\n",
            "Epoch 2/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.7115 - categorical_accuracy: 0.7749 - val_loss: 0.7638 - val_categorical_accuracy: 0.8024\n",
            "Epoch 3/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.5099 - categorical_accuracy: 0.8311 - val_loss: 0.7819 - val_categorical_accuracy: 0.8108\n",
            "Epoch 4/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.4100 - categorical_accuracy: 0.8656 - val_loss: 0.6395 - val_categorical_accuracy: 0.8406\n",
            "Epoch 5/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.3279 - categorical_accuracy: 0.8894 - val_loss: 0.6909 - val_categorical_accuracy: 0.8303\n",
            "Epoch 6/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.2749 - categorical_accuracy: 0.9079 - val_loss: 0.6959 - val_categorical_accuracy: 0.8359\n",
            "Epoch 7/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.2321 - categorical_accuracy: 0.9227 - val_loss: 0.6316 - val_categorical_accuracy: 0.8482\n",
            "Epoch 8/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.1954 - categorical_accuracy: 0.9333 - val_loss: 0.8199 - val_categorical_accuracy: 0.8398\n",
            "Epoch 9/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.1698 - categorical_accuracy: 0.9403 - val_loss: 0.6967 - val_categorical_accuracy: 0.8501\n",
            "Epoch 10/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.1439 - categorical_accuracy: 0.9490 - val_loss: 0.6508 - val_categorical_accuracy: 0.8434\n",
            "Epoch 11/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.1165 - categorical_accuracy: 0.9584 - val_loss: 0.6954 - val_categorical_accuracy: 0.8314\n",
            "Epoch 12/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.1025 - categorical_accuracy: 0.9633 - val_loss: 0.7730 - val_categorical_accuracy: 0.8551\n",
            "Epoch 13/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0846 - categorical_accuracy: 0.9708 - val_loss: 0.8360 - val_categorical_accuracy: 0.8518\n",
            "Epoch 14/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0773 - categorical_accuracy: 0.9733 - val_loss: 0.7603 - val_categorical_accuracy: 0.8555\n",
            "Epoch 15/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0644 - categorical_accuracy: 0.9775 - val_loss: 0.7526 - val_categorical_accuracy: 0.8641\n",
            "Epoch 16/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0566 - categorical_accuracy: 0.9795 - val_loss: 0.8155 - val_categorical_accuracy: 0.8467\n",
            "Epoch 17/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0520 - categorical_accuracy: 0.9825 - val_loss: 1.0215 - val_categorical_accuracy: 0.8561\n",
            "Epoch 18/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0468 - categorical_accuracy: 0.9832 - val_loss: 0.9460 - val_categorical_accuracy: 0.8596\n",
            "Epoch 19/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0401 - categorical_accuracy: 0.9860 - val_loss: 1.0393 - val_categorical_accuracy: 0.8613\n",
            "Epoch 20/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0386 - categorical_accuracy: 0.9868 - val_loss: 0.9912 - val_categorical_accuracy: 0.8596\n",
            "Epoch 21/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0326 - categorical_accuracy: 0.9889 - val_loss: 0.9843 - val_categorical_accuracy: 0.8555\n",
            "Epoch 22/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0293 - categorical_accuracy: 0.9896 - val_loss: 1.0625 - val_categorical_accuracy: 0.8553\n",
            "Epoch 23/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0247 - categorical_accuracy: 0.9918 - val_loss: 1.0637 - val_categorical_accuracy: 0.8514\n",
            "Epoch 24/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0251 - categorical_accuracy: 0.9908 - val_loss: 1.0070 - val_categorical_accuracy: 0.8484\n",
            "Epoch 25/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0227 - categorical_accuracy: 0.9922 - val_loss: 1.1202 - val_categorical_accuracy: 0.8505\n",
            "Epoch 26/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0207 - categorical_accuracy: 0.9927 - val_loss: 1.0748 - val_categorical_accuracy: 0.8424\n",
            "Epoch 27/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0225 - categorical_accuracy: 0.9924 - val_loss: 1.0907 - val_categorical_accuracy: 0.8480\n",
            "Epoch 28/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0205 - categorical_accuracy: 0.9929 - val_loss: 1.1948 - val_categorical_accuracy: 0.8578\n",
            "Epoch 29/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0192 - categorical_accuracy: 0.9937 - val_loss: 1.2511 - val_categorical_accuracy: 0.8508\n",
            "Epoch 30/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0154 - categorical_accuracy: 0.9948 - val_loss: 1.2450 - val_categorical_accuracy: 0.8529\n",
            "Epoch 31/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0178 - categorical_accuracy: 0.9937 - val_loss: 1.3877 - val_categorical_accuracy: 0.8422\n",
            "Epoch 32/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0167 - categorical_accuracy: 0.9936 - val_loss: 1.4082 - val_categorical_accuracy: 0.8480\n",
            "Epoch 33/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0148 - categorical_accuracy: 0.9948 - val_loss: 1.2814 - val_categorical_accuracy: 0.8456\n",
            "Epoch 34/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0160 - categorical_accuracy: 0.9948 - val_loss: 1.2977 - val_categorical_accuracy: 0.8501\n",
            "Epoch 35/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0155 - categorical_accuracy: 0.9947 - val_loss: 1.2819 - val_categorical_accuracy: 0.8596\n",
            "Epoch 36/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0177 - categorical_accuracy: 0.9942 - val_loss: 1.3113 - val_categorical_accuracy: 0.8398\n",
            "Epoch 37/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0148 - categorical_accuracy: 0.9954 - val_loss: 1.4195 - val_categorical_accuracy: 0.8688\n",
            "Epoch 38/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0137 - categorical_accuracy: 0.9955 - val_loss: 1.3463 - val_categorical_accuracy: 0.8467\n",
            "Epoch 39/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0125 - categorical_accuracy: 0.9957 - val_loss: 1.2517 - val_categorical_accuracy: 0.8512\n",
            "Epoch 40/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0099 - categorical_accuracy: 0.9968 - val_loss: 1.2778 - val_categorical_accuracy: 0.8516\n",
            "Epoch 41/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0131 - categorical_accuracy: 0.9956 - val_loss: 1.3742 - val_categorical_accuracy: 0.8503\n",
            "Epoch 42/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0119 - categorical_accuracy: 0.9960 - val_loss: 1.4379 - val_categorical_accuracy: 0.8649\n",
            "Epoch 43/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0086 - categorical_accuracy: 0.9973 - val_loss: 1.4592 - val_categorical_accuracy: 0.8622\n",
            "Epoch 44/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0141 - categorical_accuracy: 0.9962 - val_loss: 1.4919 - val_categorical_accuracy: 0.8348\n",
            "Epoch 45/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0096 - categorical_accuracy: 0.9967 - val_loss: 1.4314 - val_categorical_accuracy: 0.8568\n",
            "Epoch 46/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0070 - categorical_accuracy: 0.9979 - val_loss: 1.4365 - val_categorical_accuracy: 0.8568\n",
            "Epoch 47/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0116 - categorical_accuracy: 0.9965 - val_loss: 1.6058 - val_categorical_accuracy: 0.8484\n",
            "Epoch 48/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0092 - categorical_accuracy: 0.9970 - val_loss: 1.5191 - val_categorical_accuracy: 0.8467\n",
            "Epoch 49/50\n",
            "419/419 [==============================] - 57s 136ms/step - loss: 0.0098 - categorical_accuracy: 0.9965 - val_loss: 1.6459 - val_categorical_accuracy: 0.8497\n",
            "Epoch 50/50\n",
            "419/419 [==============================] - 57s 135ms/step - loss: 0.0093 - categorical_accuracy: 0.9966 - val_loss: 1.6192 - val_categorical_accuracy: 0.8499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADanzBQu17f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7c0db2-e740-4c47-8976-7a706aabbe1a"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46495, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QPdSDKJzoHK"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "new_model3 = load_model('B_Null_Basak.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwBKaGsadVKo"
      },
      "source": [
        "Testing our Model on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRxKgwvID5nL"
      },
      "source": [
        "y_prob = new_model3.predict(X_test) \n",
        "y_classes = y_prob.argmax(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lvn7o8oNfb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04042fc-63bc-4424-91ae-93073b8558d6"
      },
      "source": [
        "print(y_classes.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9894,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRrXzA36dVKo"
      },
      "source": [
        "Printing F1 Score and relevent Result Metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_z9Q-FJawlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e930aa2e-e65f-47cd-ee82-2a01889f8ac2"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, f1_score, supp = precision_recall_fscore_support(y_classes, y_test, average='weighted')\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_classes, y_test))\n",
        "#print(metrics)\n",
        "print(\"PRECISION: {0}\" .format(precision))\n",
        "print(\"RECALL: {0}\".format(recall))\n",
        "print(\"F1_SCORE : {0}\".format(f1_score))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7853   15   16    8    9  102   50   41   42   13   27   22    7   20\n",
            "    16   63  155   46]\n",
            " [   7   31    0    7    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   7    0   57    0    2    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   1   11    0   45    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   9    0   22    0   72    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [  23    0    0    0    0   99    4    5    0    1    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [  24    0    0    0    0   13   94    0    3    2    0    0    1    0\n",
            "     0    1    0    0]\n",
            " [  36    0    0    0    0   11    2   49    5    2    0    1    0    1\n",
            "     0    0    6    0]\n",
            " [  23    0    0    0    0    3    8    0   23    0    0    0    0    1\n",
            "     0    0    0    0]\n",
            " [   7    1    0    0    0    0    1    0    0   10    4    5    5    1\n",
            "     0    0    0    3]\n",
            " [   7    0    0    0    0    0    1    0    0    3    7    1    1    0\n",
            "     0    0    0    0]\n",
            " [   2    0    0    0    0    0    0    1    0    1    0    6    1    6\n",
            "     6    0    0    0]\n",
            " [   1    0    0    0    0    0    0    0    1    0    2    2    7    1\n",
            "     2    0    0    0]\n",
            " [ 109    0    0    0    0    0    0    2    0    0    0    2    2   28\n",
            "     5    0    0    0]\n",
            " [  17    0    0    0    0    0    0    0    2    0    0    1    2    9\n",
            "    32    0    0    0]\n",
            " [  11    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0   35    0    0]\n",
            " [  87    0    0    0    0    0    0    2    0    1    0    0    0    0\n",
            "     0    0  156    0]\n",
            " [  13    0    0    0    0    0    0    0    1    6    2    0    0    0\n",
            "     0    0    0   56]]\n",
            "PRECISION: 0.8904127160193469\n",
            "RECALL: 0.8752779462300384\n",
            "F1_SCORE : 0.8802136697017778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su3kgv8P7O-s"
      },
      "source": [
        "--------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF1aPhR9-MxK"
      },
      "source": [
        "## **TASK B: Applying Model on Gesture dataset (without NULL class)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ_N6_JdvZGA"
      },
      "source": [
        "Preprocessing the data from data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-LM-FdT_j5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbea88af-33ea-4a75-d62c-28546772d7cd"
      },
      "source": [
        "!python preprocess_data.py -i data/OpportunityUCIDataset.zip -o oppChallenge_gestures.data -t gestures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking dataset data/OpportunityUCIDataset.zip\n",
            "Processing dataset files ...\n",
            "... file OpportunityUCIDataset/dataset/S1-Drill.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL1.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL2.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL3.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL4.dat\n",
            "... file OpportunityUCIDataset/dataset/S1-ADL5.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-Drill.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL1.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL2.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL3.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-Drill.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL1.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL2.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL3.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL4.dat\n",
            "... file OpportunityUCIDataset/dataset/S2-ADL5.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL4.dat\n",
            "... file OpportunityUCIDataset/dataset/S3-ADL5.dat\n",
            "Final datasets with size: | train (557963, 113) | test (118750, 113) | \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiV1FCtGvZGF"
      },
      "source": [
        "Defining important variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXZPyTBA_j6B"
      },
      "source": [
        "# Hardcoded number of sensor channels employed in the OPPORTUNITY challenge\n",
        "NB_SENSOR_CHANNELS = 113\n",
        "\n",
        "# Hardcoded number of classes in the gesture recognition problem\n",
        "NUM_CLASSES = 17\n",
        "\n",
        "# Hardcoded length of the sliding window mechanism employed to segment the data\n",
        "SLIDING_WINDOW_LENGTH = 24\n",
        "\n",
        "\n",
        "# Hardcoded step of the sliding window mechanism employed to segment the data\n",
        "SLIDING_WINDOW_STEP = 12\n",
        "\n",
        "# Batch Size\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# Number filters convolutional layers\n",
        "NUM_FILTERS = 64\n",
        "\n",
        "# Size filters convolutional layers\n",
        "FILTER_SIZE = 5\n",
        "\n",
        "# Number of unit in the long short-term recurrent layers\n",
        "NUM_UNITS_LSTM = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szp6GpLQvZGF"
      },
      "source": [
        "Loading Data an segmenting data according to sliding window length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWdSB-OY_j6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba4eb0b-0294-4341-f726-caa49f5ba547"
      },
      "source": [
        "def load_dataset(filename):\n",
        "\n",
        "    f = open(filename, 'rb')\n",
        "    data = cp.load(f)\n",
        "    f.close()\n",
        "\n",
        "    X_train, y_train = data[0]\n",
        "    X_test, y_test = data[1]\n",
        "\n",
        "    print(\" ..from file {}\".format(filename))\n",
        "    print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n",
        "\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "    # The targets are casted to int8 for GPU compatibility.\n",
        "    y_train = y_train.astype(np.uint8)\n",
        "    y_test = y_test.astype(np.uint8)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "print(\"Loading data...\")\n",
        "X_train, y_train, X_test, y_test = load_dataset('data/oppChallenge_gestures.data')\n",
        "\n",
        "assert NB_SENSOR_CHANNELS == X_train.shape[1]\n",
        "def opp_sliding_window(data_x, data_y, ws, ss):\n",
        "    data_x = sliding_window(data_x,(ws,data_x.shape[1]),(ss,1))\n",
        "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,ws,ss)])\n",
        "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n",
        "\n",
        "# Sensor data is segmented using a sliding window mechanism\n",
        "X_test, y_test = opp_sliding_window(X_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
        "print(\" ..after sliding window (testing): inputs {0}, targets {1}\".format(X_test.shape, y_test.shape))\n",
        "\n",
        "# Data is reshaped since the input of the network is a 4 dimension tensor\n",
        "X_test = X_test.reshape((-1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            " ..from file data/oppChallenge_gestures.data\n",
            " ..reading instances: train (557963, 113), test (118750, 113)\n",
            " ..after sliding window (testing): inputs (9894, 24, 113), targets (9894,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUBIE2oy_j6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1694af1-746e-4096-c2cc-55b771dc70eb"
      },
      "source": [
        "X_train, y_train = opp_sliding_window(X_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
        "print(\" ..after sliding window (training): inputs {0}, targets {1}\".format(X_train.shape, y_train.shape))\n",
        "X_train = X_train.reshape((-1,SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS,1))\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ..after sliding window (training): inputs (46495, 24, 113), targets (46495,)\n",
            "(46495, 24, 113, 1)\n",
            "(46495,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndFVBBW8vZGG"
      },
      "source": [
        "Defining DeepConvLSTM Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lad-1b646NPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f135b93-3636-4c6a-c739-a8a8a90bb529"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS,1)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = tf.keras.initializers.Orthogonal()\n",
        "\n",
        "#Adding 4 CNN layers.\n",
        "model.add(layers.Conv2D(NUM_FILTERS, kernel_size=(FILTER_SIZE, 1), activation=\"relu\", kernel_initializer = initializer))\n",
        "model.add(layers.Conv2D(NUM_FILTERS, kernel_size=(FILTER_SIZE, 1), activation=\"relu\",kernel_initializer = initializer))\n",
        "model.add(layers.Conv2D(NUM_FILTERS, kernel_size=(FILTER_SIZE, 1), activation=\"relu\",kernel_initializer = initializer))\n",
        "model.add(layers.Conv2D(NUM_FILTERS, kernel_size=(FILTER_SIZE, 1), activation=\"relu\",kernel_initializer = initializer))\n",
        "model.add(layers.Permute((2,1,3)))\n",
        "model.add(layers.Reshape( (int(model.layers[4].output_shape[1]), int(model.layers[4].output_shape[2]) * int(model.layers[4].output_shape[3]))))\n",
        "\n",
        "#Adding 2 LSTM layers.\n",
        "model.add(layers.LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model.add(layers.LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "#model.add(layers.Reshape( (-1, NUM_UNITS_LSTM)))\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 20, 113, 64)       384       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 113, 64)       20544     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 12, 113, 64)       20544     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 113, 64)        20544     \n",
            "_________________________________________________________________\n",
            "permute_1 (Permute)          (None, 113, 8, 64)        0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 113, 512)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 113, 128)          328192    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 113, 128)          131584    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 14464)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 17)                245905    \n",
            "=================================================================\n",
            "Total params: 767,697\n",
            "Trainable params: 767,697\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK6tDq7l_j6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917368a4-3451-4054-c26f-ed501adab967"
      },
      "source": [
        "model.layers[4].output_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 113, 8, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCAhtKnavZGH"
      },
      "source": [
        "Encoding the training and testing data to One-Hot Encoded form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11MzIEYv_j6O"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "def prepare_targets(y_train, y_test):\n",
        "\tohe = OneHotEncoder()\n",
        "\tohe.fit(y_train)\n",
        "\ty_train_enc = ohe.transform(y_train)\n",
        "\ty_test_enc = ohe.transform(y_test)\n",
        "\treturn y_train_enc.A, y_test_enc.A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwD3_RgPvZGH"
      },
      "source": [
        "Removing NULL class values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7RnOi_t_j6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f17c614a-3cef-44f9-8e41-5cb00616202b"
      },
      "source": [
        "y_train=y_train.reshape(-1,1)\n",
        "y_test=y_test.reshape(-1,1)\n",
        "\n",
        "count = 0\n",
        "idx = []\n",
        "for i in range(0,y_train.shape[0]):\n",
        "  if(y_train[i] == 0):\n",
        "    count += 1\n",
        "    idx.append(i)\n",
        "print(len(idx))\n",
        "y_train_new = np.delete(y_train,idx)\n",
        "y_train_new = y_train_new.reshape(-1,1)\n",
        "print(y_train_new.shape)\n",
        "\n",
        "print(X_train.shape)\n",
        "X_train_new = np.delete(X_train,idx,axis=0)\n",
        "print(X_train_new.shape)\n",
        "\n",
        "count = 0\n",
        "idx = []\n",
        "for i in range(0,y_test.shape[0]):\n",
        "  if(y_test[i] == 0):\n",
        "    count += 1\n",
        "    idx.append(i)\n",
        "print(len(idx))\n",
        "y_test_new = np.delete(y_test,idx,axis = 0)\n",
        "print(y_test_new.shape)\n",
        "\n",
        "X_test_new = np.delete(X_test,idx,axis = 0)\n",
        "print(X_test_new.shape)\n",
        "\n",
        "y_train_enc, y_test_enc = prepare_targets(y_train_new, y_test_new)\n",
        "print(y_train_enc.shape)\n",
        "print(y_test_enc.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32348\n",
            "(14147, 1)\n",
            "(46495, 24, 113, 1)\n",
            "(14147, 24, 113, 1)\n",
            "8237\n",
            "(1657, 1)\n",
            "(1657, 24, 113, 1)\n",
            "(14147, 17)\n",
            "(1657, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lim-F7XvZGI"
      },
      "source": [
        "Compiling Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmssHLLhZGZc"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCVjrd58vZGI"
      },
      "source": [
        "Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H13q5gXE_j6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208829b5-9bee-4687-9c5c-7a7147decb9d"
      },
      "source": [
        "model.fit(X_train_new, y_train_enc, batch_size=BATCH_SIZE, epochs=50,validation_split=0.1)\n",
        "model.save(\"B_NoNull_Basak.h5\")\n",
        "\n",
        "#Model is already trained and loaded for testing directly."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "128/128 [==============================] - 23s 149ms/step - loss: 2.3564 - categorical_accuracy: 0.2716 - val_loss: 2.2409 - val_categorical_accuracy: 0.3731\n",
            "Epoch 2/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 1.3927 - categorical_accuracy: 0.4980 - val_loss: 1.8694 - val_categorical_accuracy: 0.4495\n",
            "Epoch 3/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 1.0575 - categorical_accuracy: 0.6095 - val_loss: 2.1049 - val_categorical_accuracy: 0.4495\n",
            "Epoch 4/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 0.8465 - categorical_accuracy: 0.6778 - val_loss: 1.8880 - val_categorical_accuracy: 0.4728\n",
            "Epoch 5/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.7166 - categorical_accuracy: 0.7219 - val_loss: 2.0134 - val_categorical_accuracy: 0.5208\n",
            "Epoch 6/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.6111 - categorical_accuracy: 0.7625 - val_loss: 2.0287 - val_categorical_accuracy: 0.5378\n",
            "Epoch 7/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 0.5615 - categorical_accuracy: 0.7850 - val_loss: 2.0771 - val_categorical_accuracy: 0.5519\n",
            "Epoch 8/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.4881 - categorical_accuracy: 0.8139 - val_loss: 2.4564 - val_categorical_accuracy: 0.5336\n",
            "Epoch 9/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.4389 - categorical_accuracy: 0.8276 - val_loss: 2.0626 - val_categorical_accuracy: 0.5541\n",
            "Epoch 10/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.3860 - categorical_accuracy: 0.8510 - val_loss: 1.8897 - val_categorical_accuracy: 0.6028\n",
            "Epoch 11/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.3513 - categorical_accuracy: 0.8678 - val_loss: 2.2612 - val_categorical_accuracy: 0.5717\n",
            "Epoch 12/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.3147 - categorical_accuracy: 0.8824 - val_loss: 2.1954 - val_categorical_accuracy: 0.5830\n",
            "Epoch 13/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.2739 - categorical_accuracy: 0.8996 - val_loss: 2.3945 - val_categorical_accuracy: 0.5724\n",
            "Epoch 14/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.2632 - categorical_accuracy: 0.9025 - val_loss: 2.5199 - val_categorical_accuracy: 0.5682\n",
            "Epoch 15/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 0.2309 - categorical_accuracy: 0.9138 - val_loss: 2.5243 - val_categorical_accuracy: 0.5753\n",
            "Epoch 16/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1943 - categorical_accuracy: 0.9311 - val_loss: 2.2515 - val_categorical_accuracy: 0.6304\n",
            "Epoch 17/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1913 - categorical_accuracy: 0.9303 - val_loss: 2.4991 - val_categorical_accuracy: 0.5972\n",
            "Epoch 18/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1658 - categorical_accuracy: 0.9377 - val_loss: 2.3565 - val_categorical_accuracy: 0.6240\n",
            "Epoch 19/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1467 - categorical_accuracy: 0.9454 - val_loss: 2.7372 - val_categorical_accuracy: 0.6120\n",
            "Epoch 20/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1459 - categorical_accuracy: 0.9468 - val_loss: 2.4908 - val_categorical_accuracy: 0.6184\n",
            "Epoch 21/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1384 - categorical_accuracy: 0.9490 - val_loss: 2.6611 - val_categorical_accuracy: 0.5802\n",
            "Epoch 22/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1343 - categorical_accuracy: 0.9514 - val_loss: 2.7990 - val_categorical_accuracy: 0.6120\n",
            "Epoch 23/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1029 - categorical_accuracy: 0.9634 - val_loss: 2.8508 - val_categorical_accuracy: 0.6071\n",
            "Epoch 24/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1022 - categorical_accuracy: 0.9645 - val_loss: 3.0402 - val_categorical_accuracy: 0.5943\n",
            "Epoch 25/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.1025 - categorical_accuracy: 0.9630 - val_loss: 2.9142 - val_categorical_accuracy: 0.5788\n",
            "Epoch 26/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 0.0868 - categorical_accuracy: 0.9700 - val_loss: 2.6995 - val_categorical_accuracy: 0.6113\n",
            "Epoch 27/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0859 - categorical_accuracy: 0.9680 - val_loss: 2.9631 - val_categorical_accuracy: 0.6226\n",
            "Epoch 28/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0747 - categorical_accuracy: 0.9722 - val_loss: 2.9009 - val_categorical_accuracy: 0.6339\n",
            "Epoch 29/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0655 - categorical_accuracy: 0.9762 - val_loss: 3.1011 - val_categorical_accuracy: 0.6127\n",
            "Epoch 30/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0762 - categorical_accuracy: 0.9718 - val_loss: 2.8641 - val_categorical_accuracy: 0.6403\n",
            "Epoch 31/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 0.0628 - categorical_accuracy: 0.9769 - val_loss: 2.9877 - val_categorical_accuracy: 0.6219\n",
            "Epoch 32/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 0.0623 - categorical_accuracy: 0.9793 - val_loss: 3.0418 - val_categorical_accuracy: 0.6473\n",
            "Epoch 33/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0520 - categorical_accuracy: 0.9805 - val_loss: 3.5635 - val_categorical_accuracy: 0.5901\n",
            "Epoch 34/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 0.0693 - categorical_accuracy: 0.9774 - val_loss: 3.2866 - val_categorical_accuracy: 0.6233\n",
            "Epoch 35/50\n",
            "128/128 [==============================] - 18s 137ms/step - loss: 0.0496 - categorical_accuracy: 0.9806 - val_loss: 3.2759 - val_categorical_accuracy: 0.6191\n",
            "Epoch 36/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0602 - categorical_accuracy: 0.9787 - val_loss: 3.0423 - val_categorical_accuracy: 0.6318\n",
            "Epoch 37/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0565 - categorical_accuracy: 0.9793 - val_loss: 3.2413 - val_categorical_accuracy: 0.6318\n",
            "Epoch 38/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0473 - categorical_accuracy: 0.9827 - val_loss: 3.2482 - val_categorical_accuracy: 0.6438\n",
            "Epoch 39/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0445 - categorical_accuracy: 0.9858 - val_loss: 3.2761 - val_categorical_accuracy: 0.6276\n",
            "Epoch 40/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0326 - categorical_accuracy: 0.9885 - val_loss: 3.2231 - val_categorical_accuracy: 0.6360\n",
            "Epoch 41/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0430 - categorical_accuracy: 0.9848 - val_loss: 3.2432 - val_categorical_accuracy: 0.6276\n",
            "Epoch 42/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0402 - categorical_accuracy: 0.9859 - val_loss: 3.4255 - val_categorical_accuracy: 0.6382\n",
            "Epoch 43/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0469 - categorical_accuracy: 0.9831 - val_loss: 3.2538 - val_categorical_accuracy: 0.6488\n",
            "Epoch 44/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0346 - categorical_accuracy: 0.9871 - val_loss: 3.3277 - val_categorical_accuracy: 0.6473\n",
            "Epoch 45/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0373 - categorical_accuracy: 0.9866 - val_loss: 3.2838 - val_categorical_accuracy: 0.6226\n",
            "Epoch 46/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0389 - categorical_accuracy: 0.9877 - val_loss: 3.3757 - val_categorical_accuracy: 0.6360\n",
            "Epoch 47/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0313 - categorical_accuracy: 0.9888 - val_loss: 3.3469 - val_categorical_accuracy: 0.6367\n",
            "Epoch 48/50\n",
            "128/128 [==============================] - 18s 139ms/step - loss: 0.0284 - categorical_accuracy: 0.9904 - val_loss: 3.4083 - val_categorical_accuracy: 0.6466\n",
            "Epoch 49/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0348 - categorical_accuracy: 0.9878 - val_loss: 3.3976 - val_categorical_accuracy: 0.6438\n",
            "Epoch 50/50\n",
            "128/128 [==============================] - 18s 138ms/step - loss: 0.0440 - categorical_accuracy: 0.9870 - val_loss: 3.3577 - val_categorical_accuracy: 0.6438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaDLRP9AvvHI"
      },
      "source": [
        "Loading Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZgXt-sRDcK6"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "new_model4 = load_model('B_NoNull_Basak.h5') #MODEL_NAME.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R9JhWEavZGI"
      },
      "source": [
        "Testing our Model on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbOVz8AEGDEH"
      },
      "source": [
        "y_prob = new_model4.predict(X_test_new) \n",
        "#print(y_prob)\n",
        "\n",
        "y_classes = y_prob.argmax(axis=-1)\n",
        "for i in range(0,y_classes.shape[0]):\n",
        "  y_classes[i] += 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srqeq-yjvZGJ"
      },
      "source": [
        "Printing F1 Score and relevent Result Metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCqVwXn8GUXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceaf1154-109b-4c28-8ad2-06fba9df2e18"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, f1_score, supp = precision_recall_fscore_support(y_classes, y_test_new, average='micro')\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_classes, y_test_new))\n",
        "#print(metrics)\n",
        "print(\"PRECISION: {0}\" .format(precision))\n",
        "print(\"RECALL: {0}\".format(recall))\n",
        "print(\"F1_SCORE : {0}\".format(f1_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 33   0  13   0   0   0   0   0   0   0   0   0   0   0   0   0   2]\n",
            " [  0  77   0   2   3   0   0   1   0   0   0   0   0   0   0   0   7]\n",
            " [ 24   1  46   2   0   0   0   0   0   0   0   0   0   0   4   6   1]\n",
            " [  0  17   0  79   0   0   0   0   0   0   0   0   0   0   1   0   0]\n",
            " [  0   0   0   0 133  11  14   1   1   0   8   1   4   0  16   2   3]\n",
            " [  0   0   0   0  29 121   3   4   0   1   0   1   1   1   3   8   3]\n",
            " [  0   0   0   0  37  11  61  14   0   0   4   0   0   1   0  15   0]\n",
            " [  0   0   1   0   4   8   7  41   0   1   0   1   0   0   0   2   0]\n",
            " [  0   0   0   0  17   7   2   2  22   9   6   4   1   0   0  33  16]\n",
            " [  0   0   0   0   4   1   0   0   5  26   4   3   0   0   0   2   4]\n",
            " [  1   0   0   0   0   0   2   1   0   1   4   0   2   0   1   0   0]\n",
            " [  0   0   0   0   0   0   0   3   1   0   5   9   2   8   0   0   0]\n",
            " [  0   0   0   0   0   0   8   7   2   0   9   5  45  16   0   0   0]\n",
            " [  0   0   0   0   0   1   3   2   2   0   0   2  12  35   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  65   2   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   9 246   0]\n",
            " [  0   0   0   0   1   0   0   1   6   4   0   0   0   0   0   1  69]]\n",
            "PRECISION: 0.6710923355461678\n",
            "RECALL: 0.6710923355461678\n",
            "F1_SCORE : 0.6710923355461678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA_TOfIQ6giH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}